{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3\n",
    "\n",
    "Date: 20-12-2020 <br>\n",
    "Nick Radunovic (s2072724) <br>\n",
    "Cheyenne Heath (s1647865) <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Stand\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:144: FutureWarning: The sklearn.metrics.scorer module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.metrics. Anything that cannot be imported from sklearn.metrics is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "C:\\Users\\Stand\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:144: FutureWarning: The sklearn.feature_selection.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.feature_selection. Anything that cannot be imported from sklearn.feature_selection is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Stand\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Imports\n",
    "import random\n",
    "import os\n",
    "random.seed(30) # set random seed for reproducibility\n",
    "\n",
    "import numpy as np\n",
    "from itertools import chain\n",
    "from collections import Counter\n",
    "import eli5\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import sklearn\n",
    "import scipy.stats\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "import sklearn_crfsuite\n",
    "from sklearn_crfsuite import scorers\n",
    "from sklearn_crfsuite import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "First, we define function that are used for preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_data(text, annotation):    \n",
    "    # parse annotation-file\n",
    "    bio_tag = []\n",
    "    with open(annotation, encoding='utf-8') as ann:\n",
    "        for l in ann:\n",
    "            l = l.split('\\t')\n",
    "            tag = l[1].split()[0]\n",
    "            if not tag == 'AnnotatorNotes':\n",
    "                string = l[2].strip()\n",
    "                bio_tag.append((tag, string))\n",
    "                \n",
    "    # parse text-file and add POS tags\n",
    "    sents = []\n",
    "    with open(text, encoding='utf-8') as fp:\n",
    "        for line in fp:\n",
    "            line = line.strip()\n",
    "            line = nltk.word_tokenize(line)\n",
    "            pos_line = add_pos_tag(line)\n",
    "            sents.append(pos_line)\n",
    "    \n",
    "    # add BIO tags and format data as follows -> (word, pos, biotag)\n",
    "    sents = add_bio_tag(sents, bio_tag)\n",
    "    \n",
    "    return sents\n",
    "\n",
    "\n",
    "def add_pos_tag(sent):\n",
    "    return nltk.pos_tag(sent)\n",
    "\n",
    "\n",
    "def add_bio_tag(sents, bio_tag):\n",
    "    \"\"\" We use 'cadec/original/' as annotation to add BIO-tags \"\"\"\n",
    "    bio_sent = []\n",
    "    for sent in sents:\n",
    "        remaining = 0\n",
    "        for i, word in enumerate(sent):\n",
    "            BIOtag = 'O'\n",
    "            for j, tag in enumerate(bio_tag):\n",
    "                target = nltk.word_tokenize(tag[1])\n",
    "                tag = tag[0]\n",
    "                count = 0\n",
    "                \n",
    "                # changes the biotag to either 'B-' or 'I-' when necessary\n",
    "                if word[0] == target[0]:\n",
    "                    for k in range(len(target)):\n",
    "                        if len(sent) > i+k and sent[i+k][0] == target[k]:\n",
    "                            count += 1\n",
    "                            \n",
    "                    # if target is found, the current word gets a 'B-' tag assigned\n",
    "                    if count == len(target):\n",
    "                        definite_tag = tag\n",
    "                        BIOtag = 'B-' + definite_tag\n",
    "                        remaining = len(target) - 1\n",
    "                        break\n",
    "            \n",
    "            # changes the biotag to 'I-' when necessary\n",
    "            if remaining > 0 and BIOtag == 'O':\n",
    "                BIOtag = 'I-' + definite_tag\n",
    "                remaining -= 1\n",
    "          \n",
    "            bio_sent.append((word[0], word[1], BIOtag))\n",
    "    return bio_sent\n",
    "\n",
    "\n",
    "# VERSION: splitting by sentence\n",
    "# def add_bio_tag(sents, bio_tag):\n",
    "#     \"\"\" We use 'cadec/original/' as annotation to add BIO-tags \"\"\"\n",
    "#     msg = []\n",
    "#     for sent in sents:\n",
    "#         bio_sent = []\n",
    "#         remaining = 0\n",
    "#         for i, word in enumerate(sent):\n",
    "#             BIOtag = 'O'\n",
    "#             for j, tag in enumerate(bio_tag):\n",
    "#                 target = nltk.word_tokenize(tag[1])\n",
    "#                 tag = tag[0]\n",
    "#                 count = 0\n",
    "                \n",
    "#                 # changes the biotag to either 'B-' or 'I-' when necessary\n",
    "#                 if word[0] == target[0]:\n",
    "#                     for k in range(len(target)):\n",
    "#                         if len(sent) > i+k and sent[i+k][0] == target[k]:\n",
    "#                             count += 1\n",
    "                            \n",
    "#                     # if target is found, the current word gets a 'B-' tag assigned\n",
    "#                     if count == len(target):\n",
    "#                         definite_tag = tag\n",
    "#                         BIOtag = 'B-' + definite_tag\n",
    "#                         remaining = len(target) - 1\n",
    "#                         break\n",
    "            \n",
    "#             # changes the biotag to 'I-' when necessary\n",
    "#             if remaining > 0 and BIOtag == 'O':\n",
    "#                 BIOtag = 'I-' + definite_tag\n",
    "#                 remaining -= 1\n",
    "          \n",
    "#             bio_sent.append((word[0], word[1], BIOtag))\n",
    "#         msg.append(bio_sent)\n",
    "#     return msg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We preprocess the data:\n",
    "\n",
    "(1) The data is parsed,\n",
    "(2) POS tags are added,\n",
    "(3) BIO tags are added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 'PRP', 'O'),\n",
       " ('feel', 'VBP', 'O'),\n",
       " ('a', 'DT', 'O'),\n",
       " ('bit', 'NN', 'B-ADR'),\n",
       " ('drowsy', 'JJ', 'I-ADR'),\n",
       " ('&', 'CC', 'O'),\n",
       " ('have', 'VBP', 'O'),\n",
       " ('a', 'DT', 'O'),\n",
       " ('little', 'JJ', 'B-ADR'),\n",
       " ('blurred', 'JJ', 'I-ADR'),\n",
       " ('vision', 'NN', 'I-ADR'),\n",
       " (',', ',', 'O'),\n",
       " ('so', 'RB', 'O'),\n",
       " ('far', 'RB', 'O'),\n",
       " ('no', 'DT', 'O'),\n",
       " ('gastric', 'JJ', 'B-ADR'),\n",
       " ('problems', 'NNS', 'I-ADR'),\n",
       " ('.', '.', 'O'),\n",
       " ('I', 'PRP', 'O'),\n",
       " (\"'ve\", 'VBP', 'O'),\n",
       " ('been', 'VBN', 'O'),\n",
       " ('on', 'IN', 'O'),\n",
       " ('Arthrotec', 'NNP', 'B-Drug'),\n",
       " ('50', 'CD', 'O'),\n",
       " ('for', 'IN', 'O'),\n",
       " ('over', 'IN', 'O'),\n",
       " ('10', 'CD', 'O'),\n",
       " ('years', 'NNS', 'O'),\n",
       " ('on', 'IN', 'O'),\n",
       " ('and', 'CC', 'O'),\n",
       " ('off', 'IN', 'O'),\n",
       " (',', ',', 'O'),\n",
       " ('only', 'RB', 'O'),\n",
       " ('taking', 'VBG', 'O'),\n",
       " ('it', 'PRP', 'O'),\n",
       " ('when', 'WRB', 'O'),\n",
       " ('I', 'PRP', 'O'),\n",
       " ('needed', 'VBD', 'O'),\n",
       " ('it', 'PRP', 'O'),\n",
       " ('.', '.', 'O'),\n",
       " ('Due', 'JJ', 'O'),\n",
       " ('to', 'TO', 'O'),\n",
       " ('my', 'PRP$', 'O'),\n",
       " ('arthritis', 'NN', 'B-Disease'),\n",
       " ('getting', 'VBG', 'O'),\n",
       " ('progressively', 'RB', 'O'),\n",
       " ('worse', 'JJR', 'O'),\n",
       " (',', ',', 'O'),\n",
       " ('to', 'TO', 'O'),\n",
       " ('the', 'DT', 'O'),\n",
       " ('point', 'NN', 'O'),\n",
       " ('where', 'WRB', 'O'),\n",
       " ('I', 'PRP', 'O'),\n",
       " ('am', 'VBP', 'O'),\n",
       " ('in', 'IN', 'O'),\n",
       " ('tears', 'NNS', 'O'),\n",
       " ('with', 'IN', 'O'),\n",
       " ('the', 'DT', 'O'),\n",
       " ('agony', 'NN', 'B-Symptom'),\n",
       " (',', ',', 'O'),\n",
       " ('gp', 'NN', 'O'),\n",
       " (\"'s\", 'POS', 'O'),\n",
       " ('started', 'VBN', 'O'),\n",
       " ('me', 'PRP', 'O'),\n",
       " ('on', 'IN', 'O'),\n",
       " ('75', 'CD', 'O'),\n",
       " ('twice', 'RB', 'O'),\n",
       " ('a', 'DT', 'O'),\n",
       " ('day', 'NN', 'O'),\n",
       " ('and', 'CC', 'O'),\n",
       " ('I', 'PRP', 'O'),\n",
       " ('have', 'VBP', 'O'),\n",
       " ('to', 'TO', 'O'),\n",
       " ('take', 'VB', 'O'),\n",
       " ('it', 'PRP', 'O'),\n",
       " ('.', '.', 'O'),\n",
       " ('every', 'DT', 'O'),\n",
       " ('day', 'NN', 'O'),\n",
       " ('for', 'IN', 'O'),\n",
       " ('the', 'DT', 'O'),\n",
       " ('next', 'JJ', 'O'),\n",
       " ('month', 'NN', 'O'),\n",
       " ('to', 'TO', 'O'),\n",
       " ('see', 'VB', 'O'),\n",
       " ('how', 'WRB', 'O'),\n",
       " ('I', 'PRP', 'O'),\n",
       " ('get', 'VBP', 'O'),\n",
       " ('on', 'IN', 'O'),\n",
       " (',', ',', 'O'),\n",
       " ('here', 'RB', 'O'),\n",
       " ('goes', 'VBZ', 'O'),\n",
       " ('.', '.', 'O'),\n",
       " ('So', 'RB', 'O'),\n",
       " ('far', 'RB', 'O'),\n",
       " ('its', 'PRP$', 'O'),\n",
       " ('been', 'VBN', 'O'),\n",
       " ('very', 'RB', 'O'),\n",
       " ('good', 'JJ', 'O'),\n",
       " (',', ',', 'O'),\n",
       " ('pains', 'VBZ', 'B-Symptom'),\n",
       " ('almost', 'RB', 'O'),\n",
       " ('gone', 'VBN', 'O'),\n",
       " (',', ',', 'O'),\n",
       " ('but', 'CC', 'O'),\n",
       " ('I', 'PRP', 'O'),\n",
       " ('feel', 'VBP', 'B-ADR'),\n",
       " ('a', 'DT', 'I-ADR'),\n",
       " ('bit', 'NN', 'I-ADR'),\n",
       " ('weird', 'JJ', 'I-ADR'),\n",
       " (',', ',', 'O'),\n",
       " ('did', 'VBD', 'O'),\n",
       " (\"n't\", 'RB', 'O'),\n",
       " ('have', 'VB', 'O'),\n",
       " ('that', 'DT', 'O'),\n",
       " ('when', 'WRB', 'O'),\n",
       " ('on', 'IN', 'O'),\n",
       " ('50', 'CD', 'O'),\n",
       " ('.', '.', 'O')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DIR_text = 'cadec/text/'\n",
    "DIR_annotation = 'cadec/original/'\n",
    "\n",
    "data = []\n",
    "for f in os.listdir(DIR_text):\n",
    "    text = DIR_text + f\n",
    "    ann = DIR_annotation + f[:-4] + '.ann'\n",
    "    m = parse_data(text, ann)\n",
    "    data.append(m)\n",
    "\n",
    "# shows first patient post\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the NER classifier\n",
    "\n",
    "We split the data into a 80% train set and a 20% test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 0.8  # specifying the % of data used as train set\n",
    "\n",
    "split = int(len(data) * train_size)\n",
    "train_data, test_data = data[:split], data[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2features(sent, i):\n",
    "    word = sent[i][0]\n",
    "    postag = sent[i][1]\n",
    "\n",
    "    features = {\n",
    "        'bias': 1.0,\n",
    "        'word.lower()': word.lower(),\n",
    "        'word[-3:]': word[-3:],\n",
    "        'word[-2:]': word[-2:],\n",
    "        'word.isupper()': word.isupper(),\n",
    "        'word.istitle()': word.istitle(),\n",
    "        'word.isdigit()': word.isdigit(),\n",
    "        'postag': postag,\n",
    "        'postag[:2]': postag[:2],\n",
    "    }\n",
    "    if i > 0:\n",
    "        word1 = sent[i-1][0]\n",
    "        postag1 = sent[i-1][1]\n",
    "        features.update({\n",
    "            '-1:word.lower()': word1.lower(),\n",
    "            '-1:word.istitle()': word1.istitle(),\n",
    "            '-1:word.isupper()': word1.isupper(),\n",
    "            '-1:postag': postag1,\n",
    "            '-1:postag[:2]': postag1[:2],\n",
    "        })\n",
    "    else:\n",
    "        features['BOS'] = True\n",
    "\n",
    "    if i < len(sent)-1:\n",
    "        word1 = sent[i+1][0]\n",
    "        postag1 = sent[i+1][1]\n",
    "        features.update({\n",
    "            '+1:word.lower()': word1.lower(),\n",
    "            '+1:word.istitle()': word1.istitle(),\n",
    "            '+1:word.isupper()': word1.isupper(),\n",
    "            '+1:postag': postag1,\n",
    "            '+1:postag[:2]': postag1[:2],\n",
    "        })\n",
    "    else:\n",
    "        features['EOS'] = True\n",
    "\n",
    "    return features\n",
    "\n",
    "def sent2features(sent):\n",
    "    return [word2features(sent, i) for i in range(len(sent))]\n",
    "\n",
    "def sent2labels(sent):\n",
    "    return [label for token, postag, label in sent]\n",
    "\n",
    "def sent2tokens(sent):\n",
    "    return [token for token, postag, label in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [sent2features(s) for s in train_data]\n",
    "y_train = [sent2labels(s) for s in train_data]\n",
    "\n",
    "X_test = [sent2features(s) for s in test_data]\n",
    "y_test = [sent2labels(s) for s in test_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Stand\\anaconda3\\lib\\site-packages\\sklearn\\base.py:197: FutureWarning: From version 0.24, get_params will raise an AttributeError if a parameter cannot be retrieved as an instance attribute. Previously it would return None.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CRF(algorithm='lbfgs', all_possible_states=None, all_possible_transitions=True,\n",
       "    averaging=None, c=None, c1=0.1, c2=0.1, calibration_candidates=None,\n",
       "    calibration_eta=None, calibration_max_trials=None, calibration_rate=None,\n",
       "    calibration_samples=None, delta=None, epsilon=None, error_sensitive=None,\n",
       "    gamma=None, keep_tempfiles=None, linesearch=None, max_iterations=100,\n",
       "    max_linesearch=None, min_freq=None, model_filename=None, num_memories=None,\n",
       "    pa_type=None, period=None, trainer_cls=None, variance=None, verbose=False)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crf = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs',\n",
    "    c1=0.1,\n",
    "    c2=0.1,\n",
    "    max_iterations=100,\n",
    "    all_possible_transitions=True\n",
    ")\n",
    "crf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B-ADR',\n",
       " 'I-ADR',\n",
       " 'B-Drug',\n",
       " 'B-Disease',\n",
       " 'B-Symptom',\n",
       " 'I-Symptom',\n",
       " 'I-Disease',\n",
       " 'I-Drug',\n",
       " 'B-Finding',\n",
       " 'I-Finding']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = list(crf.classes_)\n",
    "labels.remove('O')\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-ADR      0.732     0.686     0.708       994\n",
      "       I-ADR      0.655     0.544     0.594      1643\n",
      "   B-Disease      0.148     0.138     0.143        29\n",
      "   I-Disease      0.071     0.077     0.074        13\n",
      "      B-Drug      0.983     0.766     0.861       304\n",
      "      I-Drug      0.769     0.526     0.625        19\n",
      "   B-Finding      0.390     0.147     0.213       109\n",
      "   I-Finding      0.205     0.059     0.091       136\n",
      "   B-Symptom      0.535     0.329     0.407        70\n",
      "   I-Symptom      0.182     0.047     0.074        43\n",
      "\n",
      "   micro avg      0.688     0.557     0.616      3360\n",
      "   macro avg      0.467     0.332     0.379      3360\n",
      "weighted avg      0.666     0.557     0.603      3360\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = crf.predict(X_test)\n",
    "\n",
    "sorted_labels = sorted(\n",
    "    labels,\n",
    "    key=lambda name: (name[1:], name[0])\n",
    ")\n",
    "print(metrics.flat_classification_report(\n",
    "    y_test, y_pred, labels=sorted_labels, digits=3\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 15 candidates, totalling 45 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  45 out of  45 | elapsed:  4.6min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, error_score=nan,\n",
       "                   estimator=CRF(algorithm='lbfgs', all_possible_states=None,\n",
       "                                 all_possible_transitions=True, averaging=None,\n",
       "                                 c=None, c1=None, c2=None,\n",
       "                                 calibration_candidates=None,\n",
       "                                 calibration_eta=None,\n",
       "                                 calibration_max_trials=None,\n",
       "                                 calibration_rate=None,\n",
       "                                 calibration_samples=None, delta=None,\n",
       "                                 epsilon=None, error_sensitive=None, gamma=None,\n",
       "                                 keep_...\n",
       "                   param_distributions={'c1': <scipy.stats._distn_infrastructure.rv_frozen object at 0x0000024DA45FCB08>,\n",
       "                                        'c2': <scipy.stats._distn_infrastructure.rv_frozen object at 0x0000024DA45FC988>},\n",
       "                   pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "                   return_train_score=False,\n",
       "                   scoring=make_scorer(flat_f1_score, average=weighted, labels=['B-ADR', 'I-ADR', 'B-Drug', 'B-Disease', 'B-Symptom', 'I-Symptom', 'I-Disease', 'I-Drug', 'B-Finding', 'I-Finding']),\n",
       "                   verbose=1)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define fixed parameters and parameters to search\n",
    "crf = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs', \n",
    "    max_iterations=100, \n",
    "    all_possible_transitions=True\n",
    ")\n",
    "params_space = {\n",
    "    'c1': scipy.stats.expon(scale=0.5),\n",
    "    'c2': scipy.stats.expon(scale=0.05),\n",
    "}\n",
    "\n",
    "# use the same metric for evaluation\n",
    "f1_scorer = make_scorer(metrics.flat_f1_score, \n",
    "                        average='weighted', labels=labels)\n",
    "\n",
    "# search\n",
    "rs = RandomizedSearchCV(crf, params_space, \n",
    "                        cv=3, \n",
    "                        verbose=1, \n",
    "                        n_jobs=-1, \n",
    "                        n_iter=15, \n",
    "                        scoring=f1_scorer)\n",
    "rs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best params: {'c1': 0.4858854663877837, 'c2': 0.016422517233163236}\n",
      "best CV score: 0.5893758737287462\n",
      "model size: 0.57M\n",
      "\n",
      "weighted avg:\n",
      "precision: 0.668\n",
      "recall: 0.553\n",
      "f1-score: 0.602\n",
      "support: 3360\n"
     ]
    }
   ],
   "source": [
    "print('best params:', rs.best_params_)\n",
    "print('best CV score:', rs.best_score_)\n",
    "print('model size: {:0.2f}M'.format(rs.best_estimator_.size_ / 1000000))\n",
    "\n",
    "print(\"\\nweighted avg:\")\n",
    "crf = rs.best_estimator_\n",
    "y_pred = crf.predict(X_test)\n",
    "weighted_avg = metrics.flat_classification_report(\n",
    "    y_test, y_pred, labels=sorted_labels, digits=3, output_dict=True)['weighted avg']\n",
    "for k in weighted_avg.keys():\n",
    "    print(\"%s: %s\" % (k, round(weighted_avg[k], 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weighted avg:\n",
      "precision: 0.666\n",
      "recall: 0.557\n",
      "f1-score: 0.603\n",
      "support: 3360\n"
     ]
    }
   ],
   "source": [
    "crf = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs',\n",
    "    c1=0.1,\n",
    "    c2=0.1,\n",
    "    max_iterations=100,\n",
    "    all_possible_transitions=True\n",
    ")\n",
    "crf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = crf.predict(X_test)\n",
    "\n",
    "sorted_labels = sorted(\n",
    "    labels,\n",
    "    key=lambda name: (name[1:], name[0])\n",
    ")\n",
    "\n",
    "print(\"weighted avg:\")\n",
    "weighted_avg = metrics.flat_classification_report(\n",
    "    y_test, y_pred, labels=sorted_labels, digits=3, output_dict=True)['weighted avg']\n",
    "for k in weighted_avg.keys():\n",
    "    print(\"%s: %s\" % (k, round(weighted_avg[k], 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
