{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Mining Assignment 2\n",
    "Tasks:\n",
    "1. Download W-NUT_data.zip from the Brightspace assignment and unzip the directory. It\n",
    "contains 3 IOB files: wnut17train.conll (train), emerging.dev.conll (dev),\n",
    "emerging.test.annotated (test)\n",
    "2. The IOB files do not contain POS tags yet. Add a function to your CRFsuite script that reads\n",
    "the IOB files and adds POS tags (using an existing package for linguistic processing such as\n",
    "Spacy or NLTK). The data needs to be stored in the same way as the benchmark data from\n",
    "the tutorial (an array of triples (word,pos,biotag)).\n",
    "3. Run a baseline run (train -> test) with the features directly copied from the tutorial.\n",
    "4. Set up hyperparameter optimization using the dev set and evaluate the result on the test set.\n",
    "5. Extend the features: add a larger context (-2 .. +2 or more) and engineer a few other features\n",
    "that might be relevant for this task. Have a look at the train/dev data to get inspiration on\n",
    "potentially relevant papers.\n",
    "6. Experiment with the effect of different feature sets on the quality of the labelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\cheye\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "#Imports\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "import sklearn\n",
    "import sklearn_crfsuite\n",
    "from sklearn_crfsuite import scorers\n",
    "from sklearn_crfsuite import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to read in the data per sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_data(file):    \n",
    "    sents = []\n",
    "    with open(file, encoding='utf-8') as fp:\n",
    "        new_sent = []\n",
    "        for line in fp:\n",
    "            if (line == '\\n') or (line == '\\t\\n'):\n",
    "                #new line so end of sentence, append new_sent to sents array and clear the new_sent\n",
    "                sents.append(new_sent)\n",
    "                new_sent = []\n",
    "            else:\n",
    "                #create tuple and add to sentence\n",
    "                new_line = line.strip()\n",
    "                new_sent.append(tuple(new_line.split('\\t')))\n",
    "    return sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parse all files\n",
    "train_sents = parse_data('wnut17train.conll')\n",
    "val_sents = parse_data('emerging.dev.conll')\n",
    "test_sents =parse_data('emerging.test.annotated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_POS_tag(word_tuple):\n",
    "    #convert tuple to list\n",
    "    l = list(word_tuple)\n",
    "    \n",
    "    #insert new value at index 1\n",
    "    new_val = nltk.pos_tag(word_tuple)\n",
    "    l.insert(1, new_val[0][1])\n",
    "    \n",
    "    #convert list again to tuple\n",
    "    new_word_tuple = tuple(l)\n",
    "    return new_word_tuple  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add pos tag to each dataset, can take a few minutes\n",
    "train_sents = [[add_POS_tag(word) for word in sentence] for sentence in train_sents]\n",
    "val_sents = [[add_POS_tag(word) for word in sentence] for sentence in val_sents]\n",
    "test_sents = [[add_POS_tag(word) for word in sentence] for sentence in test_sents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Run a baseline run (train -> test) with the features directly copied from the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.15146209008241585\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "  B-corporation      0.000     0.000     0.000        66\n",
      "  I-corporation      0.000     0.000     0.000        22\n",
      "B-creative-work      0.278     0.035     0.062       142\n",
      "I-creative-work      0.333     0.041     0.073       218\n",
      "        B-group      0.304     0.042     0.074       165\n",
      "        I-group      0.316     0.086     0.135        70\n",
      "     B-location      0.396     0.240     0.299       150\n",
      "     I-location      0.360     0.096     0.151        94\n",
      "       B-person      0.555     0.154     0.241       429\n",
      "       I-person      0.517     0.229     0.317       131\n",
      "      B-product      0.667     0.016     0.031       127\n",
      "      I-product      0.385     0.040     0.072       126\n",
      "\n",
      "      micro avg      0.428     0.101     0.163      1740\n",
      "      macro avg      0.342     0.082     0.121      1740\n",
      "   weighted avg      0.412     0.101     0.151      1740\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def word2features(sent, i):\n",
    "    word = sent[i][0]\n",
    "    postag = sent[i][1]\n",
    "\n",
    "    features = {\n",
    "        'bias': 1.0,\n",
    "        'word.lower()': word.lower(),\n",
    "        'word[-3:]': word[-3:],\n",
    "        'word[-2:]': word[-2:],\n",
    "        'word.isupper()': word.isupper(),\n",
    "        'word.istitle()': word.istitle(),\n",
    "        'word.isdigit()': word.isdigit(),\n",
    "        'postag': postag,\n",
    "        'postag[:2]': postag[:2],\n",
    "    }\n",
    "    if i > 0:\n",
    "        word1 = sent[i-1][0]\n",
    "        postag1 = sent[i-1][1]\n",
    "        features.update({\n",
    "            '-1:word.lower()': word1.lower(),\n",
    "            '-1:word.istitle()': word1.istitle(),\n",
    "            '-1:word.isupper()': word1.isupper(),\n",
    "            '-1:postag': postag1,\n",
    "            '-1:postag[:2]': postag1[:2],\n",
    "        })\n",
    "    else:\n",
    "        features['BOS'] = True\n",
    "\n",
    "    if i < len(sent)-1:\n",
    "        word1 = sent[i+1][0]\n",
    "        postag1 = sent[i+1][1]\n",
    "        features.update({\n",
    "            '+1:word.lower()': word1.lower(),\n",
    "            '+1:word.istitle()': word1.istitle(),\n",
    "            '+1:word.isupper()': word1.isupper(),\n",
    "            '+1:postag': postag1,\n",
    "            '+1:postag[:2]': postag1[:2],\n",
    "        })\n",
    "    else:\n",
    "        features['EOS'] = True\n",
    "\n",
    "    return features\n",
    "\n",
    "def sent2features(sent):\n",
    "    return [word2features(sent, i) for i in range(len(sent))]\n",
    "\n",
    "def sent2labels(sent):\n",
    "    return [label for token, postag, label in sent]\n",
    "\n",
    "def sent2tokens(sent):\n",
    "    return [token for token, postag, label in sent]\n",
    "\n",
    "X_train = [sent2features(s) for s in train_sents]\n",
    "y_train = [sent2labels(s) for s in train_sents]\n",
    "\n",
    "X_test = [sent2features(s) for s in test_sents]\n",
    "y_test = [sent2labels(s) for s in test_sents]\n",
    "\n",
    "crf = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs',\n",
    "    c1=0.1,\n",
    "    c2=0.1,\n",
    "    max_iterations=100,\n",
    "    all_possible_transitions=True\n",
    ")\n",
    "crf.fit(X_train, y_train)\n",
    "\n",
    "labels = list(crf.classes_)\n",
    "labels.remove('O')\n",
    "\n",
    "y_pred = crf.predict(X_test)\n",
    "print(metrics.flat_f1_score(y_test, y_pred,\n",
    "                      average='weighted', labels=labels))\n",
    "\n",
    "sorted_labels = sorted(\n",
    "    labels,\n",
    "    key=lambda name: (name[1:], name[0])\n",
    ")\n",
    "print(metrics.flat_classification_report(\n",
    "    y_test, y_pred, labels=sorted_labels, digits=3\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up hyperparameter optimization using the dev set and evaluate the result on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
