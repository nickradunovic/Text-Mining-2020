{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "Text Mining Assignment 2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXLksxvFvBg8"
      },
      "source": [
        "# Assignment 2\n",
        "\n",
        "Date: 29-10-2020 <br>\n",
        "Nick Radunovic (s2072724) <br>\n",
        "Cheyenne Heath (s1647865) <br>\n",
        "\n",
        "Tasks:\n",
        "1. Download W-NUT_data.zip from the Brightspace assignment and unzip the directory. It\n",
        "contains 3 IOB files: wnut17train.conll (train), emerging.dev.conll (dev),\n",
        "emerging.test.annotated (test)\n",
        "2. The IOB files do not contain POS tags yet. Add a function to your CRFsuite script that reads\n",
        "the IOB files and adds POS tags (using an existing package for linguistic processing such as\n",
        "Spacy or NLTK). The data needs to be stored in the same way as the benchmark data from\n",
        "the tutorial (an array of triples (word,pos,biotag)).\n",
        "3. Run a baseline run (train -> test) with the features directly copied from the tutorial.\n",
        "4. Set up hyperparameter optimization using the dev set and evaluate the result on the test set.\n",
        "5. Extend the features: add a larger context (-2 .. +2 or more) and engineer a few other features\n",
        "that might be relevant for this task. Have a look at the train/dev data to get inspiration on\n",
        "potentially relevant papers.\n",
        "6. Experiment with the effect of different feature sets on the quality of the labelling."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Py36Z49IvBhh"
      },
      "source": [
        "#Imports\n",
        "import random\n",
        "random.seed(30) # set random seed for reproducibility\n",
        "\n",
        "import numpy as np\n",
        "from itertools import chain\n",
        "from collections import Counter\n",
        "import eli5\n",
        "\n",
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "import sklearn\n",
        "import scipy.stats\n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "import sklearn_crfsuite\n",
        "from sklearn_crfsuite import scorers\n",
        "from sklearn_crfsuite import metrics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJuVzVpKvBkE"
      },
      "source": [
        "Function to read in the data per sentence. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_5P9n5OvBlC"
      },
      "source": [
        "def parse_data(file):    \n",
        "    sents = []\n",
        "    with open(file, encoding='utf-8') as fp:\n",
        "        new_sent = []\n",
        "        for line in fp:\n",
        "            if (line == '\\n') or (line == '\\t\\n'):\n",
        "                #new line so end of sentence, append new_sent to sents array and clear the new_sent\n",
        "                sents.append(new_sent)\n",
        "                new_sent = []\n",
        "            else:\n",
        "                #create tuple and add to sentence\n",
        "                new_line = line.strip()\n",
        "                new_sent.append(tuple(new_line.split('\\t')))\n",
        "    return sents"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Odpu1CpwvBl5"
      },
      "source": [
        "#parse all files\n",
        "train_sents = parse_data('wnut17train.conll')\n",
        "dev_sents = parse_data('emerging.dev.conll')\n",
        "test_sents = parse_data('emerging.test.annotated')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LD7WwjMovBnO"
      },
      "source": [
        "Creating a function that transforms data of the form (word,pos) to the form (word,pos,biotag) for the words of each sentence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7jiMEqSivBnU"
      },
      "source": [
        "def add_POS_tag(word_pos, sent):\n",
        "    new = []\n",
        "    for i in range(len(word_pos)):\n",
        "        l = list(word_pos[i])\n",
        "        l.append(sent[i][1])\n",
        "        l = tuple(l)\n",
        "        new.append(l)\n",
        "    return new"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQk5qmVevBnj"
      },
      "source": [
        "We now add POS tags to the words of each sentence, storing the data in the format: (word,pos,biotag).\n",
        "Note, that the function pos_tag of nltk get the whole sentence as input and adds POS tags to each wordt based on both the word and the context that the word is in."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oOLZQERwvBnm"
      },
      "source": [
        "#add pos tag to each dataset, can take a few minutes\n",
        "train_sents = [add_POS_tag(nltk.pos_tag([word[0] for word in sentence]), sentence) for sentence in train_sents]\n",
        "dev_sents = [add_POS_tag(nltk.pos_tag([word[0] for word in sentence]), sentence) for sentence in dev_sents]\n",
        "test_sents = [add_POS_tag(nltk.pos_tag([word[0] for word in sentence]), sentence) for sentence in test_sents]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5BKdgrm_vBoA"
      },
      "source": [
        "3. Run a baseline run (train -> test) with the features directly copied from the tutorial."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8P90V012vBoF"
      },
      "source": [
        "def word2features(sent, i):\n",
        "    word = sent[i][0]\n",
        "    postag = sent[i][1]\n",
        "\n",
        "    features = {\n",
        "        'bias': 1.0,\n",
        "        'word.lower()': word.lower(),\n",
        "        'word[-3:]': word[-3:],\n",
        "        'word[-2:]': word[-2:],\n",
        "        'word.isupper()': word.isupper(),\n",
        "        'word.istitle()': word.istitle(),\n",
        "        'word.isdigit()': word.isdigit(),\n",
        "        'postag': postag,\n",
        "        'postag[:2]': postag[:2],\n",
        "    }\n",
        "    if i > 0:\n",
        "        word1 = sent[i-1][0]\n",
        "        postag1 = sent[i-1][1]\n",
        "        features.update({\n",
        "            '-1:word.lower()': word1.lower(),\n",
        "            '-1:word.istitle()': word1.istitle(),\n",
        "            '-1:word.isupper()': word1.isupper(),\n",
        "            '-1:postag': postag1,\n",
        "            '-1:postag[:2]': postag1[:2],\n",
        "        })\n",
        "    else:\n",
        "        features['BOS'] = True\n",
        "\n",
        "    if i < len(sent)-1:\n",
        "        word1 = sent[i+1][0]\n",
        "        postag1 = sent[i+1][1]\n",
        "        features.update({\n",
        "            '+1:word.lower()': word1.lower(),\n",
        "            '+1:word.istitle()': word1.istitle(),\n",
        "            '+1:word.isupper()': word1.isupper(),\n",
        "            '+1:postag': postag1,\n",
        "            '+1:postag[:2]': postag1[:2],\n",
        "        })\n",
        "    else:\n",
        "        features['EOS'] = True\n",
        "\n",
        "    return features\n",
        "\n",
        "def sent2features(sent):\n",
        "    return [word2features(sent, i) for i in range(len(sent))]\n",
        "\n",
        "def sent2labels(sent):\n",
        "    return [label for token, postag, label in sent]\n",
        "\n",
        "def sent2tokens(sent):\n",
        "    return [token for token, postag, label in sent]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dO-mrnrbvBo5"
      },
      "source": [
        "X_train = [sent2features(s) for s in train_sents]\n",
        "y_train = [sent2labels(s) for s in train_sents]\n",
        "\n",
        "X_dev = [sent2features(s) for s in dev_sents]\n",
        "y_dev = [sent2labels(s) for s in dev_sents]\n",
        "\n",
        "X_test = [sent2features(s) for s in test_sents]\n",
        "y_test = [sent2labels(s) for s in test_sents]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-y_izF6LvBpd"
      },
      "source": [
        "crf = sklearn_crfsuite.CRF(\n",
        "    algorithm='lbfgs',\n",
        "    c1=0.1,\n",
        "    c2=0.1,\n",
        "    max_iterations=100,\n",
        "    all_possible_transitions=True\n",
        ")\n",
        "crf.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHs1OndUvBp9"
      },
      "source": [
        "labels = list(crf.classes_)\n",
        "labels.remove('O')\n",
        "labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PXalg06zvBqN"
      },
      "source": [
        "y_pred = crf.predict(X_test)\n",
        "\n",
        "sorted_labels = sorted(\n",
        "    labels,\n",
        "    key=lambda name: (name[1:], name[0])\n",
        ")\n",
        "print(metrics.flat_classification_report(\n",
        "    y_test, y_pred, labels=sorted_labels, digits=3\n",
        "))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVpzRazkvBq7"
      },
      "source": [
        "Set up hyperparameter optimization using the dev set and evaluate the result on the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mLVuEvsmvBq_"
      },
      "source": [
        "# define fixed parameters and parameters to search\n",
        "crf = sklearn_crfsuite.CRF(\n",
        "    algorithm='lbfgs', \n",
        "    max_iterations=100, \n",
        "    all_possible_transitions=True\n",
        ")\n",
        "params_space = {\n",
        "    'c1': scipy.stats.expon(scale=0.5),\n",
        "    'c2': scipy.stats.expon(scale=0.05),\n",
        "}\n",
        "\n",
        "# use the same metric for evaluation\n",
        "f1_scorer = make_scorer(metrics.flat_f1_score, \n",
        "                        average='weighted', labels=labels)\n",
        "\n",
        "# search\n",
        "rs = RandomizedSearchCV(crf, params_space, \n",
        "                        cv=3, \n",
        "                        verbose=1, \n",
        "                        n_jobs=-1, \n",
        "                        n_iter=50, \n",
        "                        scoring=f1_scorer)\n",
        "rs.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kM6Nkee5vBrn"
      },
      "source": [
        "print('best params:', rs.best_params_)\n",
        "print('best CV score:', rs.best_score_)\n",
        "print('model size: {:0.2f}M'.format(rs.best_estimator_.size_ / 1000000))\n",
        "\n",
        "print(\"\\nweighted avg:\")\n",
        "crf = rs.best_estimator_\n",
        "y_pred = crf.predict(X_test)\n",
        "weighted_avg = metrics.flat_classification_report(\n",
        "    y_test, y_pred, labels=sorted_labels, digits=3, output_dict=True)['weighted avg']\n",
        "for k in weighted_avg.keys():\n",
        "    print(\"%s: %s\" % (k, round(weighted_avg[k], 3)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcM16lNSvBr3"
      },
      "source": [
        "#### Extend the features and perform some experiments\n",
        "\n",
        "This is the extended word2feature function that encompasses both a bigger range (-3 to +3) and a new feature: 'word.starts_with_uppercase'."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxrGLXCivBr5"
      },
      "source": [
        "def word2features_extended(sent, i):\n",
        "    word = sent[i][0]\n",
        "    postag = sent[i][1]\n",
        "\n",
        "    features = {\n",
        "        'bias': 1.0,\n",
        "        'word.lower()': word.lower(),\n",
        "        'word[-3:]': word[-3:],\n",
        "        'word[-2:]': word[-2:],\n",
        "        'word.isupper()': word.isupper(),\n",
        "        'word.istitle()': word.istitle(),\n",
        "        'word.isdigit()': word.isdigit(),\n",
        "        #'word.starts_with_uppercase': word[:1].isupper(),\n",
        "        'postag': postag,\n",
        "        'postag[:2]': postag[:2],\n",
        "    }\n",
        "    if i > 2:\n",
        "        word3 = sent[i-3][0]\n",
        "        postag3 = sent[i-3][1]\n",
        "        features.update({\n",
        "            '-3:word.lower()': word3.lower(),\n",
        "            '-3:word.istitle()': word3.istitle(),\n",
        "            '-3:word.isupper()': word3.isupper(),\n",
        "            '-3:word.isdigit()': word3.isdigit(),\n",
        "            '-3:postag': postag3,\n",
        "            '-3:postag[:2]': postag3[:2],\n",
        "        })\n",
        "    if i > 1:\n",
        "        word2 = sent[i-2][0]\n",
        "        postag2 = sent[i-2][1]\n",
        "        features.update({\n",
        "            '-2:word.lower()': word2.lower(),\n",
        "            '-2:word.istitle()': word2.istitle(),\n",
        "            '-2:word.isupper()': word2.isupper(),\n",
        "            '-2:word.isdigit()': word2.isdigit(),\n",
        "            '-2:postag': postag2,\n",
        "            '-2:postag[:2]': postag2[:2],\n",
        "        })\n",
        "    if i > 0:\n",
        "        word1 = sent[i-1][0]\n",
        "        postag1 = sent[i-1][1]\n",
        "        features.update({\n",
        "            '-1:word.lower()': word1.lower(),\n",
        "            '-1:word.istitle()': word1.istitle(),\n",
        "            '-1:word.isupper()': word1.isupper(),\n",
        "            #'word.starts_with_uppercase': word1[:1].isupper(),\n",
        "            '-1:word.isdigit()': word1.isdigit(),\n",
        "            '-1:postag': postag1,\n",
        "            '-1:postag[:2]': postag1[:2],\n",
        "        })\n",
        "    else:\n",
        "        features['BOS'] = True\n",
        "\n",
        "    if i < len(sent)-3:\n",
        "        word3 = sent[i+3][0]\n",
        "        postag3 = sent[i+3][1]\n",
        "        features.update({\n",
        "            '+3:word.lower()': word3.lower(),\n",
        "            '+3:word.istitle()': word3.istitle(),\n",
        "            '+3:word.isupper()': word3.isupper(),\n",
        "            '+3:word.isdigit()': word3.isdigit(),\n",
        "            '+3:postag': postag3,\n",
        "            '+3:postag[:2]': postag3[:2],\n",
        "        })\n",
        "    if i < len(sent)-2:\n",
        "        word2 = sent[i+2][0]\n",
        "        postag2 = sent[i+2][1]\n",
        "        features.update({\n",
        "            '+2:word.lower()': word2.lower(),\n",
        "            '+2:word.istitle()': word2.istitle(),\n",
        "            '+2:word.isupper()': word2.isupper(),\n",
        "            '+2:word.isdigit()': word2.isdigit(),\n",
        "            '+2:postag': postag2,\n",
        "            '+2:postag[:2]': postag2[:2],\n",
        "        })\n",
        "    if i < len(sent)-1:\n",
        "        word1 = sent[i+1][0]\n",
        "        postag1 = sent[i+1][1]\n",
        "        features.update({\n",
        "            '+1:word.lower()': word1.lower(),\n",
        "            '+1:word.istitle()': word1.istitle(),\n",
        "            '+1:word.isupper()': word1.isupper(),\n",
        "            '+1:word.isdigit()': word1.isdigit(),\n",
        "            '+1:postag': postag1,\n",
        "            '+1:postag[:2]': postag1[:2],\n",
        "        })\n",
        "    else:\n",
        "        features['EOS'] = True\n",
        "\n",
        "    return features\n",
        "\n",
        "def sent2features_extended(sent):\n",
        "    return [word2features_extended(sent, i) for i in range(len(sent))]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uA-K2lVBvBsD"
      },
      "source": [
        "X_dev = [sent2features_extended(s) for s in dev_sents]\n",
        "y_dev = [sent2labels(s) for s in dev_sents]\n",
        "\n",
        "X_test = [sent2features_extended(s) for s in test_sents]\n",
        "y_test = [sent2labels(s) for s in test_sents]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XjevpaGmvBsV"
      },
      "source": [
        "crf = sklearn_crfsuite.CRF(\n",
        "    algorithm='lbfgs',\n",
        "    c1=0.1,\n",
        "    c2=0.1,\n",
        "    max_iterations=100,\n",
        "    all_possible_transitions=True\n",
        ")\n",
        "crf.fit(X_dev, y_dev)\n",
        "\n",
        "y_pred = crf.predict(X_test)\n",
        "\n",
        "sorted_labels = sorted(\n",
        "    labels,\n",
        "    key=lambda name: (name[1:], name[0])\n",
        ")\n",
        "\n",
        "print(\"weighted avg:\")\n",
        "weighted_avg = metrics.flat_classification_report(\n",
        "    y_test, y_pred, labels=sorted_labels, digits=3, output_dict=True)['weighted avg']\n",
        "for k in weighted_avg.keys():\n",
        "    print(\"%s: %s\" % (k, round(weighted_avg[k], 3)))\n",
        "\n",
        "#Include gridsearch on dev set\n",
        "# define fixed parameters and parameters to search\n",
        "crf = sklearn_crfsuite.CRF(\n",
        "    algorithm='lbfgs', \n",
        "    max_iterations=100, \n",
        "    all_possible_transitions=True\n",
        ")\n",
        "params_space = {\n",
        "    'c1': scipy.stats.expon(scale=0.5),\n",
        "    'c2': scipy.stats.expon(scale=0.05),\n",
        "}\n",
        "\n",
        "# use the same metric for evaluation\n",
        "f1_scorer = make_scorer(metrics.flat_f1_score, \n",
        "                        average='weighted', labels=labels)\n",
        "\n",
        "# search\n",
        "rs = RandomizedSearchCV(crf, params_space, \n",
        "                        cv=3, \n",
        "                        verbose=1, \n",
        "                        n_jobs=-1, \n",
        "                        n_iter=10, \n",
        "                        scoring=f1_scorer)\n",
        "rs.fit(X_dev, y_dev)\n",
        "\n",
        "print('best params:', rs.best_params_)\n",
        "print(\"\\nweighted avg:\")\n",
        "crf = rs.best_estimator_\n",
        "y_pred = crf.predict(X_test)\n",
        "weighted_avg = metrics.flat_classification_report(\n",
        "    y_test, y_pred, labels=sorted_labels, digits=3, output_dict=True)['weighted avg']\n",
        "for k in weighted_avg.keys():\n",
        "    print(\"%s: %s\" % (k, round(weighted_avg[k], 3)))\n",
        "    \n",
        "# group B and I results\n",
        "sorted_labels = sorted(\n",
        "    labels,\n",
        "    key=lambda name: (name[1:], name[0])\n",
        ")\n",
        "print(metrics.flat_classification_report(\n",
        "    y_test, y_pred, labels=sorted_labels, digits=3\n",
        "))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gYlWCiyPvBtG"
      },
      "source": [
        "**Try featureset 3:**\n",
        "\n",
        "Adding:\n",
        "- range -2 .. + 2\n",
        "- prescence in list of common names in America (source: https://www.ssa.gov/oact/babynames/limits.html)\n",
        "- is hypen\n",
        "- is '@'\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9hwhYUXKvBtM"
      },
      "source": [
        "def word2features_exp3(sent, i, names):\n",
        "    word = sent[i][0]\n",
        "    postag = sent[i][1]\n",
        "\n",
        "    features = {\n",
        "        'bias': 1.0,\n",
        "        'word.lower()': word.lower(),\n",
        "        'word[-2:]': word[-2:],\n",
        "        'word.isupper()': word.isupper(),\n",
        "        'word.istitle()': word.istitle(),\n",
        "        'word.isdigit()': word.isdigit(),\n",
        "        'postag': postag,\n",
        "        'postag[:2]': postag[:2],\n",
        "        'in_names':(word in names.tolist()),\n",
        "        'is_hypen': ('@' in word),\n",
        "        'is_at':('-' in word),\n",
        "    }\n",
        "    \n",
        "    if i > 1:\n",
        "        word2 = sent[i-2][0]\n",
        "        postag2 = sent[i-2][1]\n",
        "        features.update({\n",
        "            '-2:word.lower()': word2.lower(),\n",
        "            '-2:word.istitle()': word2.istitle(),\n",
        "            '-2:word.isupper()': word2.isupper(),\n",
        "            '-2:word.isdigit()': word2.isdigit(),\n",
        "            '-2:postag': postag2,\n",
        "            '-2:postag[:2]': postag2[:2],\n",
        "            '-2:in_names':(word2 in names.tolist()),\n",
        "            '-2:is_hypen': ('@' in word2),\n",
        "            '-2:is_at':('-' in word2),\n",
        "        })\n",
        "    if i > 0:\n",
        "        word1 = sent[i-1][0]\n",
        "        postag1 = sent[i-1][1]\n",
        "        features.update({\n",
        "            '-1:word.lower()': word1.lower(),\n",
        "            '-1:word.istitle()': word1.istitle(),\n",
        "            '-1:word.isupper()': word1.isupper(),\n",
        "            '-1:word.isdigit()': word1.isdigit(),\n",
        "            '-1:postag': postag1,\n",
        "            '-1:postag[:2]': postag1[:2],\n",
        "            '-1:in_names':(word1 in names.tolist()),\n",
        "            '-1:is_hypen': ('@' in word1),\n",
        "            '-1:is_at':('-' in word1),\n",
        "        })\n",
        "    else:\n",
        "        features['BOS'] = True\n",
        "\n",
        "   \n",
        "    if i < len(sent)-2:\n",
        "        word2 = sent[i+2][0]\n",
        "        postag2 = sent[i+2][1]\n",
        "        features.update({\n",
        "            '+2:word.lower()': word2.lower(),\n",
        "            '+2:word.istitle()': word2.istitle(),\n",
        "            '+2:word.isupper()': word2.isupper(),\n",
        "            '+2:word.isdigit()': word2.isdigit(),\n",
        "            '+2:postag': postag2,\n",
        "            '+2:postag[:2]': postag2[:2],\n",
        "            '+2:in_names':(word2 in names.tolist()),\n",
        "            '+2:is_hypen': ('@' in word2),\n",
        "            '+2:is_at':('-' in word2),\n",
        "        })\n",
        "    if i < len(sent)-1:\n",
        "        word1 = sent[i+1][0]\n",
        "        postag1 = sent[i+1][1]\n",
        "        features.update({\n",
        "            '+1:word.lower()': word1.lower(),\n",
        "            '+1:word.istitle()': word1.istitle(),\n",
        "            '+1:word.isupper()': word1.isupper(),\n",
        "            '+1:word.isdigit()': word1.isdigit(),\n",
        "            '+1:postag': postag1,\n",
        "            '+1:postag[:2]': postag1[:2],\n",
        "            '+1:in_names':(word1 in names.tolist()),\n",
        "            '+1:is_hypen': ('@' in word1),\n",
        "            '+1:is_at':('-' in word1),\n",
        "        })\n",
        "    else:\n",
        "        features['EOS'] = True\n",
        "\n",
        "    return features\n",
        "\n",
        "def sent2features_exp3(sent):\n",
        "    names = np.genfromtxt('yob2010.txt', delimiter=\",\", usecols = (0), dtype='unicode')\n",
        "    return [word2features_exp3(sent, i, names) for i in range(len(sent))]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9fOFQd-XvBtd"
      },
      "source": [
        "X_dev = [sent2features_exp3(s) for s in dev_sents]\n",
        "y_dev = [sent2labels(s) for s in dev_sents]\n",
        "\n",
        "X_test = [sent2features_exp3(s) for s in test_sents]\n",
        "y_test = [sent2labels(s) for s in test_sents]\n",
        "\n",
        "crf = sklearn_crfsuite.CRF(\n",
        "    algorithm='lbfgs',\n",
        "    c1=0.1,\n",
        "    c2=0.1,\n",
        "    max_iterations=100,\n",
        "    all_possible_transitions=True\n",
        ")\n",
        "crf.fit(X_dev, y_dev)\n",
        "\n",
        "y_pred = crf.predict(X_test)\n",
        "\n",
        "sorted_labels = sorted(\n",
        "    labels,\n",
        "    key=lambda name: (name[1:], name[0])\n",
        ")\n",
        "\n",
        "print('experiment with feature set 3')\n",
        "print(\"weighted avg:\")\n",
        "weighted_avg = metrics.flat_classification_report(\n",
        "    y_test, y_pred, labels=sorted_labels, digits=3, output_dict=True)['weighted avg']\n",
        "for k in weighted_avg.keys():\n",
        "    print(\"%s: %s\" % (k, round(weighted_avg[k], 3)))\n",
        "    \n",
        "#Including gridsearch\n",
        "# define fixed parameters and parameters to search\n",
        "crf = sklearn_crfsuite.CRF(\n",
        "    algorithm='lbfgs', \n",
        "    max_iterations=100, \n",
        "    all_possible_transitions=True\n",
        ")\n",
        "params_space = {\n",
        "    'c1': scipy.stats.expon(scale=0.5),\n",
        "    'c2': scipy.stats.expon(scale=0.05),\n",
        "}\n",
        "\n",
        "# use the same metric for evaluation\n",
        "f1_scorer = make_scorer(metrics.flat_f1_score, \n",
        "                        average='weighted', labels=labels)\n",
        "\n",
        "# search\n",
        "rs = RandomizedSearchCV(crf, params_space, \n",
        "                        cv=3, \n",
        "                        verbose=1, \n",
        "                        n_jobs=-1, \n",
        "                        n_iter=10, \n",
        "                        scoring=f1_scorer)\n",
        "rs.fit(X_dev, y_dev)\n",
        "\n",
        "print('best params:', rs.best_params_)\n",
        "print(\"\\nweighted avg:\")\n",
        "crf = rs.best_estimator_\n",
        "y_pred = crf.predict(X_test)\n",
        "weighted_avg = metrics.flat_classification_report(\n",
        "    y_test, y_pred, labels=sorted_labels, digits=3, output_dict=True)['weighted avg']\n",
        "for k in weighted_avg.keys():\n",
        "    print(\"%s: %s\" % (k, round(weighted_avg[k], 3)))\n",
        "    \n",
        "# group B and I results\n",
        "sorted_labels = sorted(\n",
        "    labels,\n",
        "    key=lambda name: (name[1:], name[0])\n",
        ")\n",
        "print(metrics.flat_classification_report(\n",
        "    y_test, y_pred, labels=sorted_labels, digits=3\n",
        "))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cLIam92vBt8"
      },
      "source": [
        "Try featureset 4:\n",
        "Same as above with with 3 words, but with lower added back in"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QoRZeduzvBuC"
      },
      "source": [
        "def word2features_exp3(sent, i, names):\n",
        "    word = sent[i][0]\n",
        "    postag = sent[i][1]\n",
        "\n",
        "    features = {\n",
        "        'bias': 1.0,\n",
        "        'word[-2:]': word[-2:],\n",
        "        'word.lower()': word.lower(),\n",
        "        'word.isupper()': word.isupper(),\n",
        "        'word.istitle()': word.istitle(),\n",
        "        'word.isdigit()': word.isdigit(),\n",
        "        'postag': postag,\n",
        "        'postag[:2]': postag[:2],\n",
        "        'in_names':(word in names.tolist()),\n",
        "        'is_hypen': ('@' in word),\n",
        "        'is_at':('-' in word),\n",
        "    }\n",
        "    if i > 2:\n",
        "        word3 = sent[i-3][0]\n",
        "        postag3 = sent[i-3][1]\n",
        "        features.update({\n",
        "            '-3:word.lower()': word3.lower(),\n",
        "            '-3:word.istitle()': word3.istitle(),\n",
        "            '-3:word.isupper()': word3.isupper(),\n",
        "            '-3:word.isdigit()': word3.isdigit(),\n",
        "            '-3:postag': postag3,\n",
        "            '-3:postag[:2]': postag3[:2],\n",
        "            '-3:is_hypen': ('@' in word3),\n",
        "            '-3:is_at':('-' in word3),\n",
        "        })\n",
        "    \n",
        "    if i > 1:\n",
        "        word2 = sent[i-2][0]\n",
        "        postag2 = sent[i-2][1]\n",
        "        features.update({\n",
        "            '-2:word.lower()': word2.lower(),\n",
        "            '-2:word.istitle()': word2.istitle(),\n",
        "            '-2:word.isupper()': word2.isupper(),\n",
        "            '-2:word.isdigit()': word2.isdigit(),\n",
        "            '-2:postag': postag2,\n",
        "            '-2:postag[:2]': postag2[:2],\n",
        "            '-2:in_names':(word2 in names.tolist()),\n",
        "            '-2:is_hypen': ('@' in word2),\n",
        "            '-2:is_at':('-' in word2),\n",
        "        })\n",
        "    if i > 0:\n",
        "        word1 = sent[i-1][0]\n",
        "        postag1 = sent[i-1][1]\n",
        "        features.update({\n",
        "            '-1:word.lower()': word1.lower(),\n",
        "            '-1:word.istitle()': word1.istitle(),\n",
        "            '-1:word.isupper()': word1.isupper(),\n",
        "            '-1:word.isdigit()': word1.isdigit(),\n",
        "            '-1:postag': postag1,\n",
        "            '-1:postag[:2]': postag1[:2],\n",
        "            '-1:in_names':(word1 in names.tolist()),\n",
        "            '-1:is_hypen': ('@' in word1),\n",
        "            '-1:is_at':('-' in word1),\n",
        "        })\n",
        "    else:\n",
        "        features['BOS'] = True\n",
        "    \n",
        "    if i < len(sent)-3:\n",
        "        word3 = sent[i+3][0]\n",
        "        postag3 = sent[i+3][1]\n",
        "        features.update({\n",
        "            '+3:word.lower()': word3.lower(),\n",
        "            '+3:word.istitle()': word3.istitle(),\n",
        "            '+3:word.isupper()': word3.isupper(),\n",
        "            '+3:word.isdigit()': word3.isdigit(),\n",
        "            '+3:postag': postag3,\n",
        "            '+3:postag[:2]': postag3[:2],\n",
        "            '+3:is_hypen': ('@' in word3),\n",
        "            '+3:is_at':('-' in word3),\n",
        "        })\n",
        "\n",
        "   \n",
        "    if i < len(sent)-2:\n",
        "        word2 = sent[i+2][0]\n",
        "        postag2 = sent[i+2][1]\n",
        "        features.update({\n",
        "            '+2:word.lower()': word2.lower(),\n",
        "            '+2:word.istitle()': word2.istitle(),\n",
        "            '+2:word.isupper()': word2.isupper(),\n",
        "            '+2:word.isdigit()': word2.isdigit(),\n",
        "            '+2:postag': postag2,\n",
        "            '+2:postag[:2]': postag2[:2],\n",
        "            '+2:in_names':(word2 in names.tolist()),\n",
        "            '+2:is_hypen': ('@' in word2),\n",
        "            '+2:is_at':('-' in word2),\n",
        "        })\n",
        "    if i < len(sent)-1:\n",
        "        word1 = sent[i+1][0]\n",
        "        postag1 = sent[i+1][1]\n",
        "        features.update({\n",
        "            '+1:word.lower()': word1.lower(),\n",
        "            '+1:word.istitle()': word1.istitle(),\n",
        "            '+1:word.isupper()': word1.isupper(),\n",
        "            '+1:word.isdigit()': word1.isdigit(),\n",
        "            '+1:postag': postag1,\n",
        "            '+1:postag[:2]': postag1[:2],\n",
        "            '+1:in_names':(word1 in names.tolist()),\n",
        "            '+1:is_hypen': ('@' in word1),\n",
        "            '+1:is_at':('-' in word1),\n",
        "        })\n",
        "    else:\n",
        "        features['EOS'] = True\n",
        "\n",
        "    return features\n",
        "\n",
        "def sent2features_exp3(sent):\n",
        "    names = np.genfromtxt('yob2010.txt', delimiter=\",\", usecols = (0), dtype='unicode')\n",
        "    return [word2features_exp3(sent, i, names) for i in range(len(sent))]\n",
        "\n",
        "X_dev = [sent2features_exp3(s) for s in dev_sents]\n",
        "y_dev = [sent2labels(s) for s in dev_sents]\n",
        "\n",
        "X_test = [sent2features_exp3(s) for s in test_sents]\n",
        "y_test = [sent2labels(s) for s in test_sents]\n",
        "\n",
        "crf = sklearn_crfsuite.CRF(\n",
        "    algorithm='lbfgs',\n",
        "    c1=0.1,\n",
        "    c2=0.1,\n",
        "    max_iterations=100,\n",
        "    all_possible_transitions=True\n",
        ")\n",
        "crf.fit(X_dev, y_dev)\n",
        "\n",
        "y_pred = crf.predict(X_test)\n",
        "\n",
        "sorted_labels = sorted(\n",
        "    labels,\n",
        "    key=lambda name: (name[1:], name[0])\n",
        ")\n",
        "\n",
        "print('experiment with feature set 4')\n",
        "print(\"weighted avg:\")\n",
        "weighted_avg = metrics.flat_classification_report(\n",
        "    y_test, y_pred, labels=sorted_labels, digits=3, output_dict=True)['weighted avg']\n",
        "for k in weighted_avg.keys():\n",
        "    print(\"%s: %s\" % (k, round(weighted_avg[k], 3)))\n",
        "\n",
        "\n",
        "#Including gridsearch\n",
        "# define fixed parameters and parameters to search\n",
        "crf = sklearn_crfsuite.CRF(\n",
        "    algorithm='lbfgs', \n",
        "    max_iterations=100, \n",
        "    all_possible_transitions=True\n",
        ")\n",
        "params_space = {\n",
        "    'c1': scipy.stats.expon(scale=0.5),\n",
        "    'c2': scipy.stats.expon(scale=0.05),\n",
        "}\n",
        "\n",
        "# use the same metric for evaluation\n",
        "f1_scorer = make_scorer(metrics.flat_f1_score, \n",
        "                        average='weighted', labels=labels)\n",
        "\n",
        "# search\n",
        "rs = RandomizedSearchCV(crf, params_space, \n",
        "                        cv=3, \n",
        "                        verbose=1, \n",
        "                        n_jobs=-1, \n",
        "                        n_iter=10, \n",
        "                        scoring=f1_scorer)\n",
        "rs.fit(X_dev, y_dev)\n",
        "\n",
        "print('best params:', rs.best_params_)\n",
        "print(\"\\nweighted avg:\")\n",
        "crf = rs.best_estimator_\n",
        "y_pred = crf.predict(X_test)\n",
        "weighted_avg = metrics.flat_classification_report(\n",
        "    y_test, y_pred, labels=sorted_labels, digits=3, output_dict=True)['weighted avg']\n",
        "for k in weighted_avg.keys():\n",
        "    print(\"%s: %s\" % (k, round(weighted_avg[k], 3)))\n",
        "    \n",
        "# group B and I results\n",
        "sorted_labels = sorted(\n",
        "    labels,\n",
        "    key=lambda name: (name[1:], name[0])\n",
        ")\n",
        "print(metrics.flat_classification_report(\n",
        "    y_test, y_pred, labels=sorted_labels, digits=3\n",
        "))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXsT9amZvBuW"
      },
      "source": [
        "Because we see that the baseline model does not see the difference between B- and I- categories very well, we lookad at the data to see if we could find any patterns. We saw that the use of prepositions and articles is very common for to appear near the B- categorie, so we might think that that would have an influence.\n",
        "\n",
        "- prepositions for place and time like: at, on & in\n",
        "- articles  a, an & the\n",
        "\n",
        "To check ik a words is a verb or a noun with the help of nltk.corpus wordnet:\n",
        "is_verb = (wn.synsets(word)[0].pos() == 'v')\n",
        "is_noun = (wn.synsets(word)[0].pos() == 'n')\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJ2yFKCDvBug"
      },
      "source": [
        "from nltk.corpus import wordnet as wn\n",
        "nltk.download('wordnet')\n",
        "prepos = ['at', 'on', 'in']\n",
        "articles = ['a','an','the']\n",
        "\n",
        "def word2features_exp3(sent, i):\n",
        "    word = sent[i][0]\n",
        "    postag = sent[i][1]\n",
        "\n",
        "    features = {\n",
        "        'bias': 1.0,\n",
        "        'word.lower()': word.lower(),\n",
        "        'word[-3:]': word[-3:],\n",
        "        'word[-2:]': word[-2:],\n",
        "        'word.isupper()': word.isupper(),\n",
        "        'word.istitle()': word.istitle(),\n",
        "        'word.isdigit()': word.isdigit(),\n",
        "        'postag': postag,\n",
        "        'postag[:2]': postag[:2],\n",
        "        'is_prepos': (word in prepos),\n",
        "        'is_atricle': (word in articles),\n",
        "        #'is_verb':(wn.synsets(word)[0].pos() == 'v'),\n",
        "        #'is_noun':(wn.synsets(word)[0].pos() == 'n'),\n",
        "\n",
        "        \n",
        "    }\n",
        "    if i > 2:\n",
        "        word3 = sent[i-3][0]\n",
        "        postag3 = sent[i-3][1]\n",
        "        features.update({\n",
        "            '-3:word.lower()': word3.lower(),\n",
        "            '-3:word.istitle()': word3.istitle(),\n",
        "            '-3:word.isupper()': word3.isupper(),\n",
        "            '-3:word.isdigit()': word3.isdigit(),\n",
        "            '-3:postag': postag3,\n",
        "            '-3:postag[:2]': postag3[:2],\n",
        "            '-3:is_prepos': (word3 in prepos),\n",
        "            '-3:is_atricle': (word3 in articles),\n",
        "            #'-3:is_verb':(wn.synsets(word3)[0].pos() == 'v'),\n",
        "            #'-3:is_noun':(wn.synsets(word3)[0].pos() == 'n'),\n",
        "        })\n",
        "    if i > 1:\n",
        "        word2 = sent[i-2][0]\n",
        "        postag2 = sent[i-2][1]\n",
        "        features.update({\n",
        "            '-2:word.lower()': word2.lower(),\n",
        "            '-2:word.istitle()': word2.istitle(),\n",
        "            '-2:word.isupper()': word2.isupper(),\n",
        "            '-2:word.isdigit()': word2.isdigit(),\n",
        "            '-2:postag': postag2,\n",
        "            '-2:postag[:2]': postag2[:2],\n",
        "            '-2:is_prepos': (word2 in prepos),\n",
        "            '-2:is_atricle': (word2 in articles),\n",
        "            #'-2:is_verb':(wn.synsets(word2)[0].pos() == 'v'),\n",
        "            #'-2:is_noun':(wn.synsets(word2)[0].pos() == 'n'),\n",
        "        })\n",
        "    if i > 0:\n",
        "        word1 = sent[i-1][0]\n",
        "        postag1 = sent[i-1][1]\n",
        "        features.update({\n",
        "            '-1:word.lower()': word1.lower(),\n",
        "            '-1:word.istitle()': word1.istitle(),\n",
        "            '-1:word.isupper()': word1.isupper(),\n",
        "            #'word.starts_with_uppercase': word1[:1].isupper(),\n",
        "            '-1:word.isdigit()': word1.isdigit(),\n",
        "            '-1:postag': postag1,\n",
        "            '-1:postag[:2]': postag1[:2],\n",
        "            '-1:is_prepos': (word1 in prepos),\n",
        "            '-1:is_atricle': (word1 in articles),\n",
        "            #'-1:is_verb':(wn.synsets(word1)[0].pos() == 'v'),\n",
        "            #'-1:is_noun':(wn.synsets(word1)[0].pos() == 'n'),\n",
        "        })\n",
        "    else:\n",
        "        features['BOS'] = True\n",
        "\n",
        "    if i < len(sent)-3:\n",
        "        word3 = sent[i+3][0]\n",
        "        postag3 = sent[i+3][1]\n",
        "        features.update({\n",
        "            '+3:word.lower()': word3.lower(),\n",
        "            '+3:word.istitle()': word3.istitle(),\n",
        "            '+3:word.isupper()': word3.isupper(),\n",
        "            '+3:word.isdigit()': word3.isdigit(),\n",
        "            '+3:postag': postag3,\n",
        "            '+3:postag[:2]': postag3[:2],\n",
        "            '+3:is_prepos': (word3 in prepos),\n",
        "            '+3:is_atricle': (word3 in articles),\n",
        "            #'+3:is_verb':(wn.synsets(word3)[0].pos() == 'v'),\n",
        "            #'+3:is_noun':(wn.synsets(word3)[0].pos() == 'n'),\n",
        "        })\n",
        "    if i < len(sent)-2:\n",
        "        word2 = sent[i+2][0]\n",
        "        postag2 = sent[i+2][1]\n",
        "        features.update({\n",
        "            '+2:word.lower()': word2.lower(),\n",
        "            '+2:word.istitle()': word2.istitle(),\n",
        "            '+2:word.isupper()': word2.isupper(),\n",
        "            '+2:word.isdigit()': word2.isdigit(),\n",
        "            '+2:postag': postag2,\n",
        "            '+2:is_prepos': (word2 in prepos),\n",
        "            '+2:postag[:2]': postag2[:2],\n",
        "            '+2:is_atricle': (word2 in articles),\n",
        "            #'+2:is_verb':(wn.synsets(word2)[0].pos() == 'v'),\n",
        "            #'+2:is_noun':(wn.synsets(word2)[0].pos() == 'n'),\n",
        "        })\n",
        "    if i < len(sent)-1:\n",
        "        word1 = sent[i+1][0]\n",
        "        postag1 = sent[i+1][1]\n",
        "        features.update({\n",
        "            '+1:word.lower()': word1.lower(),\n",
        "            '+1:word.istitle()': word1.istitle(),\n",
        "            '+1:word.isupper()': word1.isupper(),\n",
        "            '+1:word.isdigit()': word1.isdigit(),\n",
        "            '+1:postag': postag1,\n",
        "            '+1:postag[:2]': postag1[:2],\n",
        "            '+1:is_prepos': (word1 in prepos),\n",
        "            '+1:is_atricle': (word1 in articles),\n",
        "            #'+1:is_verb':(wn.synsets(word1)[0].pos() == 'v'),\n",
        "            #'+1:is_noun':(wn.synsets(word1)[0].pos() == 'n'),\n",
        "        })\n",
        "    else:\n",
        "        features['EOS'] = True\n",
        "\n",
        "    return features\n",
        "\n",
        "def sent2features_exp3(sent):\n",
        "    names = np.genfromtxt('yob2010.txt', delimiter=\",\", usecols = (0), dtype='unicode')\n",
        "    return [word2features_exp3(sent, i) for i in range(len(sent))]\n",
        "\n",
        "X_dev = [sent2features_exp3(s) for s in dev_sents]\n",
        "y_dev = [sent2labels(s) for s in dev_sents]\n",
        "\n",
        "X_test = [sent2features_exp3(s) for s in test_sents]\n",
        "y_test = [sent2labels(s) for s in test_sents]\n",
        "\n",
        "crf = sklearn_crfsuite.CRF(\n",
        "    algorithm='lbfgs',\n",
        "    c1=0.1,\n",
        "    c2=0.1,\n",
        "    max_iterations=100,\n",
        "    all_possible_transitions=True\n",
        ")\n",
        "crf.fit(X_dev, y_dev)\n",
        "\n",
        "y_pred = crf.predict(X_test)\n",
        "\n",
        "sorted_labels = sorted(\n",
        "    labels,\n",
        "    key=lambda name: (name[1:], name[0])\n",
        ")\n",
        "\n",
        "print('experiment with feature set 4')\n",
        "print(\"weighted avg:\")\n",
        "weighted_avg = metrics.flat_classification_report(\n",
        "    y_test, y_pred, labels=sorted_labels, digits=3, output_dict=True)['weighted avg']\n",
        "for k in weighted_avg.keys():\n",
        "    print(\"%s: %s\" % (k, round(weighted_avg[k], 3)))\n",
        "\n",
        "\n",
        "#Including gridsearch\n",
        "# define fixed parameters and parameters to search\n",
        "crf = sklearn_crfsuite.CRF(\n",
        "    algorithm='lbfgs', \n",
        "    max_iterations=100, \n",
        "    all_possible_transitions=True\n",
        ")\n",
        "params_space = {\n",
        "    'c1': scipy.stats.expon(scale=0.5),\n",
        "    'c2': scipy.stats.expon(scale=0.05),\n",
        "}\n",
        "\n",
        "# use the same metric for evaluation\n",
        "f1_scorer = make_scorer(metrics.flat_f1_score, \n",
        "                        average='weighted', labels=labels)\n",
        "\n",
        "# search\n",
        "rs = RandomizedSearchCV(crf, params_space, \n",
        "                        cv=3, \n",
        "                        verbose=1, \n",
        "                        n_jobs=-1, \n",
        "                        n_iter=10, \n",
        "                        scoring=f1_scorer)\n",
        "rs.fit(X_dev, y_dev)\n",
        "\n",
        "print('best params:', rs.best_params_)\n",
        "print(\"\\nweighted avg:\")\n",
        "crf = rs.best_estimator_\n",
        "y_pred = crf.predict(X_test)\n",
        "weighted_avg = metrics.flat_classification_report(\n",
        "    y_test, y_pred, labels=sorted_labels, digits=3, output_dict=True)['weighted avg']\n",
        "for k in weighted_avg.keys():\n",
        "    print(\"%s: %s\" % (k, round(weighted_avg[k], 3)))\n",
        "    \n",
        "# group B and I results\n",
        "sorted_labels = sorted(\n",
        "    labels,\n",
        "    key=lambda name: (name[1:], name[0])\n",
        ")\n",
        "print(metrics.flat_classification_report(\n",
        "    y_test, y_pred, labels=sorted_labels, digits=3\n",
        "))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}