{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1\n",
    "\n",
    "Date: 03-10-2020 <br>\n",
    "Nick Radunovic (s2072724) <br>\n",
    "Cheyenne Heath (s1647865) <br>\n",
    "\n",
    "The goal of this assignment is to categorize the 20newsgroup dataset using several classifiers. For each classifier, multiple features are tested and compared.\n",
    "After analyzing the obtained results, it should be clear what classifier peforms best and with what features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, the train and test data are fetched from the 20newsgroup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 11314\n",
      "Test: 7532\n",
      "Ratio: 60.03 (train) / 39.97 (test)\n"
     ]
    }
   ],
   "source": [
    "# Download the train dataset into 'twenty_train'\n",
    "twenty_train = fetch_20newsgroups(subset='train', shuffle=True, random_state=42)\n",
    "twenty_test = fetch_20newsgroups(subset='test', shuffle=True, random_state=42)\n",
    "\n",
    "# Print global statistic of data\n",
    "print('Train:', len(twenty_train.data))\n",
    "print('Test:', len(twenty_test.data))\n",
    "total = len(twenty_train.data) + len(twenty_test.data)\n",
    "print('Ratio:', round(len(twenty_train.data)/total*100, 2), '(train) /' , round(len(twenty_test.data)/total*100, 2), '(test)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, The pipelines for each classifier are initialized.\n",
    "The classifiers that will be compared are Naive Bayes, Support Vector Machine (SVM) and Random Forest.\n",
    "Each classifier will be initialized three times utilizing each time a different feature. The different features that are considered are CountVectorizer, TF and TF-IDF. Note, that 9 different pipelines are initialized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize pipelines for NB\n",
    "NB_clf_counts = Pipeline([('vect', CountVectorizer()), \n",
    "                          ('clf', MultinomialNB()),\n",
    "                         ])\n",
    "\n",
    "NB_clf_tf = Pipeline([('vect', CountVectorizer()), \n",
    "                      ('tf', TfidfTransformer(use_idf=False)),\n",
    "                      ('clf', MultinomialNB()),\n",
    "                      ])\n",
    "\n",
    "NB_clf_tfidf = Pipeline([('vect', CountVectorizer()), \n",
    "                         ('tfidf', TfidfTransformer()),\n",
    "                         ('clf', MultinomialNB()),\n",
    "                         ])\n",
    "\n",
    "#Initialize pipelines for LSVM\n",
    "LSVM_clf_counts = Pipeline([('vect', CountVectorizer()), \n",
    "                          ('clf', LinearSVC()),\n",
    "                         ])\n",
    "\n",
    "LSVM_clf_tf = Pipeline([('vect', CountVectorizer()), \n",
    "                      ('tf', TfidfTransformer(use_idf=False)),\n",
    "                      ('clf', LinearSVC()),\n",
    "                      ])\n",
    "\n",
    "LSVM_clf_tfidf = Pipeline([('vect', CountVectorizer()), \n",
    "                         ('tfidf', TfidfTransformer()),\n",
    "                         ('clf', LinearSVC()),\n",
    "                         ])\n",
    "\n",
    "#Initialize pipelines for RF\n",
    "RF_clf_counts = Pipeline([('vect', CountVectorizer()), \n",
    "                          ('clf', RandomForestClassifier()),\n",
    "                         ])\n",
    "\n",
    "RF_clf_tf = Pipeline([('vect', CountVectorizer()), \n",
    "                      ('tf', TfidfTransformer(use_idf=False)),\n",
    "                      ('clf', RandomForestClassifier()),\n",
    "                     ])\n",
    "\n",
    "RF_clf_tfidf = Pipeline([('vect', CountVectorizer()), \n",
    "                         ('tfidf', TfidfTransformer()),\n",
    "                         ('clf', RandomForestClassifier()),\n",
    "                        ])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The third step is to fit the classifiers and make new predictions.\n",
    "Remark, that the metrics of precision/recall can be understand in the following way: <br>\n",
    "\n",
    "__High precision, low recall__ = returns very few results, but most of its predicted labels are correct when compared to the actual labels. <br>\n",
    "__Low precision, high recall__ = returns many results, but most of its predicted labels are incorrect when compared to the actual labels.\n",
    "\n",
    "Below, the quality results of Naive Bayes are calculated for each of the three features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes\n",
      "Counts:\n",
      "accuracy: 0.77\n",
      "precision: 0.76\n",
      "recall: 0.77\n",
      "f1-score: 0.75\n",
      "\n",
      "TF:\n",
      "accuracy: 0.71\n",
      "precision: 0.79\n",
      "recall: 0.71\n",
      "f1-score: 0.69\n",
      "\n",
      "TF-IDF:\n",
      "accuracy: 0.77\n",
      "precision: 0.82\n",
      "recall: 0.77\n",
      "f1-score: 0.77\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes\n",
    "print(\"Naive Bayes\")\n",
    "\n",
    "features = [\"Counts:\", \"TF:\", \"TF-IDF:\"]\n",
    "naive_bayes = [NB_clf_counts, NB_clf_tf, NB_clf_tfidf]\n",
    "for i in range(len(naive_bayes)):\n",
    "    nb = naive_bayes[i].fit(twenty_train.data, twenty_train.target)\n",
    "    predicted = nb.predict(twenty_test.data)\n",
    "    avg_metrics = metrics.classification_report(twenty_test.target, predicted,\n",
    "                                    target_names=twenty_test.target_names, output_dict=True)['weighted avg']\n",
    "    print(features[i])\n",
    "    print(\"accuracy:\", round(np.mean(predicted == twenty_test.target), 2))\n",
    "    for metric in list(avg_metrics.keys())[:3]:\n",
    "        print(\"%s: %r\" % (metric, round(avg_metrics[metric], 2)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the same manner, the quality results of SVM are calculated for each of the three features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Support Vector Machine\n",
      "Counts:\n",
      "accuracy: 0.79\n",
      "precision: 0.79\n",
      "recall: 0.79\n",
      "f1-score: 0.78\n",
      "\n",
      "TF:\n",
      "accuracy: 0.83\n",
      "precision: 0.83\n",
      "recall: 0.83\n",
      "f1-score: 0.82\n",
      "\n",
      "TF-IDF:\n",
      "accuracy: 0.85\n",
      "precision: 0.85\n",
      "recall: 0.85\n",
      "f1-score: 0.85\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Linear Support Vector Machine\n",
    "print(\"Linear Support Vector Machine\")\n",
    "\n",
    "support_vector_machine = [LSVM_clf_counts, LSVM_clf_tf, LSVM_clf_tfidf]\n",
    "for i in range(len(support_vector_machine)):\n",
    "    svm = support_vector_machine[i].fit(twenty_train.data, twenty_train.target)\n",
    "    predicted = svm.predict(twenty_test.data)\n",
    "    avg_metrics = metrics.classification_report(twenty_test.target, predicted,\n",
    "                                    target_names=twenty_test.target_names, output_dict=True)['weighted avg']\n",
    "    print(features[i])\n",
    "    print(\"accuracy:\", round(np.mean(predicted == twenty_test.target), 2))\n",
    "    for metric in list(avg_metrics.keys())[:3]:\n",
    "        print(\"%s: %r\" % (metric, round(avg_metrics[metric], 2)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, the quality results of Random Forest are calculated for each of the three features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "Counts:\n",
      "accuracy: 0.77\n",
      "precision: 0.78\n",
      "recall: 0.77\n",
      "f1-score: 0.76\n",
      "\n",
      "TF:\n",
      "accuracy: 0.75\n",
      "precision: 0.76\n",
      "recall: 0.75\n",
      "f1-score: 0.75\n",
      "\n",
      "TF-IDF:\n",
      "accuracy: 0.76\n",
      "precision: 0.77\n",
      "recall: 0.76\n",
      "f1-score: 0.76\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Random Forest\n",
    "print(\"Random Forest\")\n",
    "\n",
    "random_forest = [RF_clf_counts, RF_clf_tf, RF_clf_tfidf]\n",
    "for i in range(len(random_forest)):\n",
    "    rf = random_forest[i].fit(twenty_train.data, twenty_train.target)\n",
    "    predicted = rf.predict(twenty_test.data)\n",
    "    avg_metrics = metrics.classification_report(twenty_test.target, predicted,\n",
    "                                    target_names=twenty_test.target_names, output_dict=True)['weighted avg']\n",
    "    print(features[i])\n",
    "    print(\"accuracy:\", round(np.mean(predicted == twenty_test.target), 2))\n",
    "    for metric in list(avg_metrics.keys())[:3]:\n",
    "        print(\"%s: %r\" % (metric, round(avg_metrics[metric], 2)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classifier SVM appeared to be the most accurate in categorizing the 20newsgroup dataset, achieving an accuracy of 85% when utilizing the TF-IDF feature. <br>\n",
    "However, some other parameters could be tweaked as well which may lead to an increase in accuracy. <br>\n",
    "Below, the quality results of the classifier SVM are calculated and shown using different parameters values of the CountVectorizer feature. Only different parameter values for the SVM classifier are tested, as previous tests showed that SVM obtains the best categorization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Support Vector Machine (CountVectorizer)\n",
      "\n",
      "Lowercase True\n",
      "accuracy: 0.79\n",
      "precision: 0.79\n",
      "recall: 0.79\n",
      "f1-score: 0.78\n",
      "\n",
      "Lowercase False\n",
      "accuracy: 0.79\n",
      "precision: 0.79\n",
      "recall: 0.79\n",
      "f1-score: 0.79\n",
      "\n",
      "Stop-words = 'english'\n",
      "accuracy: 0.8\n",
      "precision: 0.8\n",
      "recall: 0.8\n",
      "f1-score: 0.8\n",
      "\n",
      "analyzer = 'word', ngram_range = (1,2)\n",
      "accuracy: 0.81\n",
      "precision: 0.81\n",
      "recall: 0.81\n",
      "f1-score: 0.81\n",
      "\n",
      "analyzer = 'word', ngram_range = (2,2)\n",
      "accuracy: 0.75\n",
      "precision: 0.75\n",
      "recall: 0.75\n",
      "f1-score: 0.75\n",
      "\n",
      "analyzer = 'char', ngram_range = (1,1)\n",
      "accuracy: 0.12\n",
      "precision: 0.33\n",
      "recall: 0.12\n",
      "f1-score: 0.1\n",
      "\n",
      "analyzer = 'char', ngram_range = (1,2)\n",
      "accuracy: 0.6\n",
      "precision: 0.62\n",
      "recall: 0.6\n",
      "f1-score: 0.6\n",
      "\n",
      "analyzer = 'char', ngram_range = (2,2)\n",
      "accuracy: 0.61\n",
      "precision: 0.61\n",
      "recall: 0.61\n",
      "f1-score: 0.61\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Linear Support Vector Machine\n",
    "print(\"Linear Support Vector Machine (CountVectorizer)\\n\")\n",
    "\n",
    "LSVM_clf_counts_lowercase_true = Pipeline([('vect', CountVectorizer()), \n",
    "                          ('clf', LinearSVC()),\n",
    "                         ])\n",
    "LSVM_clf_counts_lowercase_false = Pipeline([('vect', CountVectorizer(lowercase=False)), \n",
    "                          ('clf', LinearSVC()),\n",
    "                         ])\n",
    "LSVM_clf_counts_lowercase_stop_words_english = Pipeline([('vect', CountVectorizer(stop_words='english')), \n",
    "                          ('clf', LinearSVC()),\n",
    "                         ])\n",
    "LSVM_clf_counts_lowercase_analyzer_word_12 = Pipeline([('vect', CountVectorizer(ngram_range = (1,2))), \n",
    "                          ('clf', LinearSVC()),\n",
    "                         ])\n",
    "LSVM_clf_counts_lowercase_analyzer_word_22 = Pipeline([('vect', CountVectorizer(ngram_range = (2,2))), \n",
    "                          ('clf', LinearSVC()),\n",
    "                         ])\n",
    "LSVM_clf_counts_lowercase_analyzer_char_11 = Pipeline([('vect', CountVectorizer(ngram_range = (1,1), analyzer = 'char')), \n",
    "                          ('clf', LinearSVC()),\n",
    "                         ])\n",
    "LSVM_clf_counts_lowercase_analyzer_char_12 = Pipeline([('vect', CountVectorizer(ngram_range = (1,2), analyzer = 'char')), \n",
    "                          ('clf', LinearSVC()),\n",
    "                         ])\n",
    "LSVM_clf_counts_lowercase_analyzer_char_22 = Pipeline([('vect', CountVectorizer(ngram_range = (2,2), analyzer = 'char')), \n",
    "                          ('clf', LinearSVC()),\n",
    "                         ])\n",
    "\n",
    "LSVM = [LSVM_clf_counts_lowercase_true, LSVM_clf_counts_lowercase_false, LSVM_clf_counts_lowercase_stop_words_english,\n",
    "             LSVM_clf_counts_lowercase_analyzer_word_12, LSVM_clf_counts_lowercase_analyzer_word_22,\n",
    "             LSVM_clf_counts_lowercase_analyzer_char_11, LSVM_clf_counts_lowercase_analyzer_char_12,\n",
    "             LSVM_clf_counts_lowercase_analyzer_char_22]\n",
    "\n",
    "LSVM_labels = [\"Lowercase True\", \"Lowercase False\", \"Stop-words = 'english'\", \"analyzer = 'word', ngram_range = (1,2)\",\n",
    "               \"analyzer = 'word', ngram_range = (2,2)\", \"analyzer = 'char', ngram_range = (1,1)\", \n",
    "               \"analyzer = 'char', ngram_range = (1,2)\", \"analyzer = 'char', ngram_range = (2,2)\"]\n",
    "\n",
    "for i in range(len(LSVM)):\n",
    "    svm = LSVM[i].fit(twenty_train.data, twenty_train.target)\n",
    "    predicted = svm.predict(twenty_test.data)\n",
    "    avg_metrics = metrics.classification_report(twenty_test.target, predicted,\n",
    "                                                target_names=twenty_test.target_names, output_dict=True)['weighted avg']\n",
    "    print(LSVM_labels[i])\n",
    "    print(\"accuracy:\", round(np.mean(predicted == twenty_test.target), 2))\n",
    "    for metric in list(avg_metrics.keys())[:3]:\n",
    "        print(\"%s: %r\" % (metric, round(avg_metrics[metric], 2)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to obtain the parameter combination that leads to the best categorization, a grid search is being used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'vect__ngram_range': [(1, 1), (1, 2), (1, 3), (2, 2), (2, 3), (3, 3)],\n",
    "    'vect__lowercase': (True, False),\n",
    "    'vect__stop_words': (None, 'english'),\n",
    "    'vect__analyzer': ('word', 'char', 'char_wb'),\n",
    "    'vect__max_features': (None, 10, 50, 100, 500, 1000, 5000, 10000),\n",
    "}\n",
    "\n",
    "gs_clf = GridSearchCV(LSVM_clf_counts_lowercase_true, parameters, cv=5, n_jobs=-1)\n",
    "gs_clf = gs_clf.fit(twenty_train.data, twenty_train.target)\n",
    "\n",
    "print(\"Best mean score:\", gs_clf.best_score_)\n",
    "\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"%s: %r\" % (param_name, gs_clf.best_params_[param_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result above shows the feature combination of CountVectorizer that produces the best results.\n",
    "To see wether or not the accuracy of SVM is higher with these parameter values, we computer the accuracy of the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.83\n"
     ]
    }
   ],
   "source": [
    "LSVM = Pipeline([('vect', CountVectorizer(ngram_range = (1,3), stop_words='english')), \n",
    "                          ('clf', LinearSVC()),\n",
    "                         ])\n",
    "\n",
    "svm = LSVM.fit(twenty_train.data, twenty_train.target)\n",
    "predicted = svm.predict(twenty_test.data)\n",
    "print(\"accuracy:\", round(np.mean(predicted == twenty_test.target), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the accuracy using these parameter values is higher than than the accuracy obtained using default values.\n",
    "However, the accuracy of the classifier when using ID-IDF is still the highest."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
